{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glue_crawler(database_name, table_prefix, s3_target_path):\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    try:\n",
    "        # Create Glue Database if it doesn't exist\n",
    "        glue_client.create_database(\n",
    "            DatabaseInput={'Name': database_name}\n",
    "        )\n",
    "        \n",
    "        # Create crawler configuration\n",
    "        crawler_name = f\"{database_name}_crawler\"\n",
    "        \n",
    "        response = glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role='AWSGlueServiceRole',  # Make sure this role exists with proper permissions\n",
    "            DatabaseName=database_name,\n",
    "            TablePrefix=table_prefix,\n",
    "            Targets={\n",
    "                'S3Targets': [\n",
    "                    {'Path': s3_target_path}\n",
    "                ]\n",
    "            },\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Start the crawler\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except glue_client.exceptions.AlreadyExistsException:\n",
    "        print(f\"Crawler {crawler_name} already exists\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/starting crawler: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "    sc = SparkContext()\n",
    "    glueContext = GlueContext(sc)\n",
    "    spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "    job.init(args['JOB_NAME'], args)\n",
    "    \n",
    "    # Parameters for your data\n",
    "    database_name = \"your_database_name\"\n",
    "    table_prefix = \"your_table_prefix\"\n",
    "    s3_target_path = \"s3://your-bucket/your-directory/\"\n",
    "    \n",
    "    # Create and start the crawler\n",
    "    crawler_success = create_glue_crawler(\n",
    "        database_name=database_name,\n",
    "        table_prefix=table_prefix,\n",
    "        s3_target_path=s3_target_path\n",
    "    )\n",
    "    \n",
    "    if crawler_success:\n",
    "        print(\"Data is now available in Athena\")\n",
    "    else:\n",
    "        print(\"Failed to make data available in Athena\")\n",
    "    \n",
    "    job.commit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
